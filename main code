! kaggle datasets download -d mohamedhanyyy/chest-ctscan-images
import os
import shutil
# Paths for source and destination
base_path = '/content/Data'
new_base_path = '/content/chest-ctscan-ima'
# Loop through each split (train, valid, test)
for split in ['train', 'valid', 'test']:
 split_path = os.path.join(base_path, split)
 new_split_path = os.path.join(new_base_path, split)
 os.makedirs(new_split_path, exist_ok=True)
 # Move each class folder to the new structure
 for class_folder in os.listdir(split_path):
 class_path = os.path.join(split_path, class_folder)
 if os.path.isdir(class_path):
 # Determine the class name without additional metadata in the name
 class_name = class_folder.split('_')[0] # Simplifies name to "adenocarcinoma", etc.
 new_class_path = os.path.join(new_split_path, class_name)
 os.makedirs(new_class_path, exist_ok=True)
 # Move all images from the old class folder to the new class folder
 for img_file in os.listdir(class_path):
 shutil.move(os.path.join(class_path, img_file), new_class_path)
import os

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import xgboost as xgb
from sklearn.utils import class_weight
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.metrics import classification_report
import joblib
# Constants
IMG_SIZE = 224
BATCH_SIZE = 32
NUM_CLASSES = 4
EPOCHS = 100
class CapsuleLayer(layers.Layer):
 def init(self, num_capsules, dim_capsules, routings=3, **kwargs):
 super(CapsuleLayer, self).init(kwargs
 self.num_capsules = num_capsules
 self.dim_capsules = dim_capsules
 self.routings = routings
 def build(self, input_shape):
 self.W = self.add_weight(
 shape=[input_shape[-1], self.num_capsules * self.dim_capsules],
 initializer='glorot_uniform',
 trainable=True
 )
 def call(self, inputs):
 u_hat = tf.einsum('bij,jk->bik', inputs, self.W)
 u_hat = tf.reshape(u_hat, [-1, inputs.shape[1], self.num_capsules, self.dim_capsules])

 b = tf.zeros(shape=[tf.shape(inputs)[0], inputs.shape[1], self.num_capsules, 1])
 for i in range(self.routings):
 c = tf.nn.softmax(b, axis=2)
 s = tf.reduce_sum(c * u_hat, axis=1, keepdims=True)
 v = self.squash(s)

if i < self.routings - 1:
 b += tf.reduce_sum(u_hat * v, axis=-1, keepdims=True)
 return tf.squeeze(v, axis=1)
 def squash(self, vectors):
 s_squared_norm = tf.reduce_sum(tf.square(vectors), axis=-1, keepdims=True)
 return (s_squared_norm / (1 + s_squared_norm)) * (vectors / tf.sqrt(s_squared_norm +
1e-8))
def create_hybrid_model():
 # Base CNN (EfficientNetB0) - only freeze early layers
 base_model = EfficientNetB0(weights='imagenet', include_top=False,
input_shape=(IMG_SIZE, IMG_SIZE, 3))
 for layer in base_model.layers[:-20]: # Unfreeze the last 20 layers
 layer.trainable = True
 inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
 x = base_model(inputs)
 x = layers.GlobalAveragePooling2D()(x)
 # Modify the capsule layer
 x = layers.Reshape((-1, 128))(x)
 capsule = CapsuleLayer(num_capsules=NUM_CLASSES, dim_capsules=16,
routings=5)(x) # Increase dim_capsules and routings
 # Add dense layers for classification
 x = layers.Flatten()(capsule)
 x = layers.Dense(512, activation='relu')(x)
 x = layers.Dropout(0.5)(x) # Increase dropout for regularization
 x = layers.Dense(256, activation='relu')(x)
 outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)

 model = models.Model(inputs=inputs, outputs=outputs)
 return model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
def preprocess_data(train_data_path, val_data_path):
 datagen = ImageDataGenerator(
 rescale=1./255,
 rotation_range=30,
 width_shift_range=0.2,
 height_shift_range=0.2,
 horizontal_flip=True,
 shear_range=0.15,
 zoom_range=0.2,
 brightness_range=[0.8, 1.2]
 )
 train_generator = datagen.flow_from_directory(
 train_data_path,
 target_size=(IMG_SIZE, IMG_SIZE),
 batch_size=BATCH_SIZE,
 class_mode='categorical'
 )
 val_generator = datagen.flow_from_directory(
 val_data_path,
 target_size=(IMG_SIZE, IMG_SIZE),
 batch_size=BATCH_SIZE,
 class_mode='categorical'
 )
 return train_generator, val_generator
def train_model(model, train_generator, val_generator, class_weights):
 model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
 history = model.fit(
 train_generator,
 epochs=EPOCHS,

 validation_data=val_generator,
 class_weight=class_weights, # Pass the calculated class weights here
 callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,
restore_best_weights=True)]
 )
 return history
from sklearn.model_selection import cross_val_score
def evaluate_xgboost(xgb_model, features, labels):
 # Perform cross-validation
 cv_scores = cross_val_score(xgb_model, features, labels, cv=5, scoring='f1_weighted')
 print(f"Cross-Validation F1 Scores: {cv_scores}")
 print(f"Average Cross-Validation F1 Score: {np.mean(cv_scores)}")
def extract_features(model, generator):
 features, labels = [], []
 for i in range(len(generator)):
 batch_x, batch_y = generator[i]
 batch_features = model.predict(batch_x)
 features.extend(batch_features)
 labels.extend(np.argmax(batch_y, axis=1))
 return np.array(features), np.array(labels)
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
def train_xgboost(features, labels):
 # Split data for training and testing
 X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2,
random_state=42)
 # Define and train the XGBoost model
 xgb_model = xgb.XGBClassifier(
 objective='multi:softmax',
 num_class=NUM_CLASSES,
 learning_rate=0.05, # Reduce learning rate
 max_depth=4, # Shallower tree
 n_estimators=200, # More estimators for improved performance
 subsample=0.8, # Use subsampling for regularization

 colsample_bytree=0.8 # Use column subsampling
 )
 xgb_model.fit(X_train, y_train)
 # Return the model along with test features and labels for evaluation
 return xgb_model, X_test, y_test
rom imblearn.over_sampling import SMOTE
def balance_features(features, labels):
 smote = SMOTE()
 balanced_features, balanced_labels = smote.fit_resample(features, labels)
 return balanced_features, balanced_labels
def main():
 # Define the dataset paths for train and validation sets
 train_data_path = '/content/chest-ctscan-ima/train'
 val_data_path = '/content/chest-ctscan-ima/valid'
 # Step 1: Create and compile the hybrid model
 model = create_hybrid_model()
 # Step 2: Preprocess the data to get train and validation generators
 train_generator, val_generator = preprocess_data(train_data_path, val_data_path)
 # Step 3: Calculate class weights based on the training data
 class_labels = np.unique(train_generator.classes)
 class_weights = class_weight.compute_class_weight(
 class_weight='balanced',
 classes=class_labels,
 y=train_generator.classes
 )
 class_weights_dict = dict(enumerate(class_weights))
 # Step 4: Train the CNN model with class weights
 history = train_model(model, train_generator, val_generator,
class_weights=class_weights_dict)

 # Step 5: Save the CNN model for Streamlit app usage
 model.save('cnn_capsule_model.h5')
 # Step 6: Extract features from the trained CNN model
 train_features, train_labels = extract_features(model, train_generator)
 # Step 7: Balance the dataset with SMOTE
 balanced_features, balanced_labels = balance_features(train_features, train_labels)
 # Step 8: Hyperparameter tuning for XGBoost and train
 best_xgb_model, X_test, y_test = train_xgboost(balanced_features, balanced_labels)
 # Step 9: Save the tuned XGBoost model
 joblib.dump(best_xgb_model, 'xgb_model.joblib')
 # Step 10: Evaluate XGBoost model with cross-validation
 evaluate_xgboost(best_xgb_model, balanced_features, balanced_labels)
 # Step 11: Evaluate XGBoost model on the test set
 print("XGBoost Test Evaluation:")
 y_pred = best_xgb_model.predict(X_test)
 print(classification_report(y_test, y_pred))
if name == "main":
 main()
